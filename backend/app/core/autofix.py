# backend/app/core/autofix.py

from __future__ import annotations

from typing import Any, Dict, List, Mapping, Tuple


def _get_basic(profile: Mapping[str, Any]) -> Dict[str, Any]:
    return profile.get("basic_profile") or {}


def _infer_numeric_columns(profile: Mapping[str, Any]) -> List[str]:
    basic = _get_basic(profile)
    inferred_types = basic.get("inferred_types") or {}
    column_stats = basic.get("column_stats") or {}

    numeric_cols: List[str] = []

    for col, t in inferred_types.items():
        t_str = str(t).lower()
        if any(key in t_str for key in ("int", "float", "double", "number")):
            numeric_cols.append(col)

    for col, stats in column_stats.items():
        if col in numeric_cols:
            continue
        t = str(stats.get("inferred_type", "")).lower()
        if any(key in t for key in ("int", "float", "double", "number")):
            numeric_cols.append(col)

    return sorted(list(dict.fromkeys(numeric_cols)))


def _infer_date_columns(profile: Mapping[str, Any]) -> List[str]:
    basic = _get_basic(profile)
    inferred_types = basic.get("inferred_types") or {}

    date_cols: List[str] = []
    for col, t in inferred_types.items():
        t_str = str(t).lower()
        if "date" in t_str or "time" in t_str:
            date_cols.append(col)

    # heuristicâ€”also treat name-based hints
    for col in inferred_types.keys():
        name_lower = col.lower()
        if any(key in name_lower for key in ("date", "dt", "timestamp")):
            if col not in date_cols:
                date_cols.append(col)

    return sorted(list(dict.fromkeys(date_cols)))


def _infer_categorical_columns(profile: Mapping[str, Any], numeric_cols: List[str]) -> List[str]:
    basic = _get_basic(profile)
    inferred_types = basic.get("inferred_types") or {}
    column_stats = basic.get("column_stats") or {}

    numeric_set = set(numeric_cols)
    categorical: List[str] = []

    for col, t in inferred_types.items():
        if col in numeric_set:
            continue
        t_str = str(t).lower()
        if "string" in t_str or "object" in t_str or "category" in t_str:
            categorical.append(col)

    for col, stats in column_stats.items():
        if col in numeric_set or col in categorical:
            continue
        t = str(stats.get("inferred_type", "")).lower()
        if "string" in t or "object" in t or "category" in t:
            categorical.append(col)

    return sorted(list(dict.fromkeys(categorical)))


def _infer_email_columns(profile: Mapping[str, Any]) -> List[str]:
    basic = _get_basic(profile)
    inferred_types = basic.get("inferred_types") or {}
    email_cols: List[str] = []

    for col, t in inferred_types.items():
        t_str = str(t).lower()
        if "email" in t_str:
            email_cols.append(col)

    for col in inferred_types.keys():
        name_lower = col.lower()
        if "email" in name_lower and col not in email_cols:
            email_cols.append(col)

    return sorted(list(dict.fromkeys(email_cols)))


def _infer_pii_columns(pii_result: Mapping[str, Any]) -> List[str]:
    pii_cols = pii_result.get("pii_columns") or []
    out: List[str] = []
    for item in pii_cols:
        col = item.get("column")
        if col:
            out.append(col)
    return sorted(list(dict.fromkeys(out)))


# ---------------------------------------------------------------------------
# Plan builder
# ---------------------------------------------------------------------------

def _build_plan(
    dataset_name: str,
    profile: Mapping[str, Any],
    pii_result: Mapping[str, Any],
    outlier_result: Mapping[str, Any],
) -> Dict[str, Any]:
    numeric_cols = _infer_numeric_columns(profile)
    date_cols = _infer_date_columns(profile)
    cat_cols = _infer_categorical_columns(profile, numeric_cols)
    email_cols = _infer_email_columns(profile)
    pii_cols = _infer_pii_columns(pii_result)

    header = f'''"""
AutoFix script for dataset: {dataset_name}

This script was generated by DataLakeQ.
You can edit steps or disable ones you don't need.
"""

import pandas as pd
import numpy as np

INPUT_PATH = "input.csv"          # TODO: change to your raw CSV path
OUTPUT_PATH = "autofixed_output.csv"  # TODO: change if needed

df = pd.read_csv(INPUT_PATH)

# Column groups inferred from profiling
NUMERIC_COLUMNS = {numeric_cols}
DATE_COLUMNS = {date_cols}
CATEGORICAL_COLUMNS = {cat_cols}
EMAIL_COLUMNS = {email_cols}
PII_COLUMNS = {pii_cols}

'''

    footer = '''
# --- Save result ------------------------------------------------------------

df.to_csv(OUTPUT_PATH, index=False)
print(f"Saved cleaned data to {OUTPUT_PATH}")
'''

    steps: List[Dict[str, Any]] = []

    # 1) Missing numeric imputation
    if numeric_cols:
        steps.append(
            {
                "id": "missing_numeric_impute",
                "label": "Fill missing numeric values with median",
                "category": "missing",
                "enabled": True,
                "description": "For each numeric column, fill NaNs with the column median.",
                "code": '''# 1) Fill missing numeric values with median
for col in NUMERIC_COLUMNS:
    if col in df.columns:
        median_value = df[col].median()
        df[col] = df[col].fillna(median_value)
''',
            }
        )

    # 2) Missing categorical imputation
    if cat_cols:
        steps.append(
            {
                "id": "missing_categorical_impute",
                "label": "Fill missing categorical values with mode",
                "category": "missing",
                "enabled": True,
                "description": "For each categorical column, fill NaNs with the most frequent value.",
                "code": '''# 2) Fill missing categorical values with mode
for col in CATEGORICAL_COLUMNS:
    if col in df.columns:
        mode_series = df[col].mode(dropna=True)
        if not mode_series.empty:
            mode_value = mode_series.iloc[0]
            df[col] = df[col].fillna(mode_value)
''',
            }
        )

    # 3) Outlier clipping for numeric columns (IQR)
    if numeric_cols:
        steps.append(
            {
                "id": "outlier_clip_iqr",
                "label": "Clip numeric outliers using IQR (1.5x)",
                "category": "outliers",
                "enabled": True,
                "description": "Winsorize extreme values beyond 1.5 * IQR range for numeric columns.",
                "code": '''# 3) Clip numeric outliers using IQR
for col in NUMERIC_COLUMNS:
    if col in df.columns:
        q1 = df[col].quantile(0.25)
        q3 = df[col].quantile(0.75)
        iqr = q3 - q1
        if iqr == 0:
            continue
        lower = q1 - 1.5 * iqr
        upper = q3 + 1.5 * iqr
        df[col] = df[col].clip(lower=lower, upper=upper)
''',
            }
        )

    # 4) Date parsing
    if date_cols:
        steps.append(
            {
                "id": "date_parse_iso",
                "label": "Parse date/time columns to ISO-8601",
                "category": "dates",
                "enabled": True,
                "description": "Parse DATE_COLUMNS using pandas.to_datetime and format as ISO strings.",
                "code": '''# 4) Normalize date / datetime columns to ISO-8601 strings
for col in DATE_COLUMNS:
    if col in df.columns:
        df[col] = pd.to_datetime(df[col], errors="coerce")
        df[col] = df[col].dt.strftime("%Y-%m-%dT%H:%M:%S").fillna(df[col].astype(str))
''',
            }
        )

    # 5) Email normalization
    if email_cols:
        steps.append(
            {
                "id": "email_normalize",
                "label": "Normalize email columns (trim + lowercase)",
                "category": "strings",
                "enabled": True,
                "description": "Strip whitespace and lowercase email addresses for consistency.",
                "code": '''# 5) Normalize email columns
for col in EMAIL_COLUMNS:
    if col in df.columns:
        df[col] = (
            df[col]
            .astype(str)
            .str.strip()
            .str.lower()
            .replace({"nan": np.nan})
        )
''',
            }
        )

    # 6) PII masking / hashing
    if pii_cols:
        steps.append(
            {
                "id": "pii_mask",
                "label": "Mask PII columns (hash values)",
                "category": "pii",
                "enabled": False,  # be conservative by default
                "description": "Hash PII columns using SHA256 to reduce exposure of raw values.",
                "code": '''# 6) Mask PII columns by hashing values (SHA256)
import hashlib

for col in PII_COLUMNS:
    if col in df.columns:
        def _hash_value(v):
            if pd.isna(v):
                return v
            s = str(v).encode("utf-8")
            return hashlib.sha256(s).hexdigest()

        df[col] = df[col].apply(_hash_value)
''',
            }
        )

    plan: Dict[str, Any] = {
        "header": header,
        "footer": footer,
        "steps": steps,
    }
    return plan


def _assemble_script(plan: Mapping[str, Any]) -> str:
    header = plan.get("header", "")
    footer = plan.get("footer", "")
    steps = plan.get("steps") or []
    body_parts: List[str] = []

    for step in steps:
        if step.get("enabled"):
            code = step.get("code") or ""
            if code:
                body_parts.append(code)

    body = "\n".join(body_parts)
    script = header.rstrip() + "\n\n" + body.rstrip() + "\n\n" + footer.lstrip()
    return script


# ---------------------------------------------------------------------------
# Public API used by main.py
# ---------------------------------------------------------------------------

def build_autofix(
    df: Any,
    dataset_name: str,
    profile: Mapping[str, Any],
    pii_result: Mapping[str, Any],
    outlier_result: Mapping[str, Any],
) -> Tuple[Dict[str, Any], str]:
    """
    Main AutoFix engine.

    Returns:
      (plan, script)

    plan:
      {
        "header": "...",
        "footer": "...",
        "steps": [
          {
            "id": "missing_numeric_impute",
            "label": "...",
            "category": "...",
            "enabled": true,
            "description": "...",
            "code": "..."
          },
          ...
        ]
      }

    script:
      Full Python script assembled from header + enabled step code + footer.
    """
    plan = _build_plan(
        dataset_name=dataset_name,
        profile=profile,
        pii_result=pii_result,
        outlier_result=outlier_result,
    )
    script = _assemble_script(plan)
    return plan, script


# Backwards compatibility alias
def generate_autofix(
    df: Any,
    dataset_name: str,
    profile: Mapping[str, Any],
    pii_result: Mapping[str, Any],
    outlier_result: Mapping[str, Any],
) -> Tuple[Dict[str, Any], str]:
    return build_autofix(df, dataset_name, profile, pii_result, outlier_result)
